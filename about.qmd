---
title: "Paul Beard FIA"
subtitle: "Life Actuary and R Developer"
format: 
  html:
    fig-pos: "H"
fig-width: 8.5
fig-asp: 0.618
out-width: "70%" 
fig-align: center
tbl-cap-location: top
fig-cap-location: top
execute:
  echo: false
toc: true
---

```{r}
#| label: setup
#| include: false

library(tidyverse)
library(gt)
library(lubridate)
library(plotly)

data_loc <- "./about/Data"
data_filename <- "Work History Summary.xlsx"

# Read in data
data_filepath <- paste(data_loc,"/",data_filename,sep="")
read_all_excel_sheets <- function(filename, tibble = FALSE) {
  sheets <- readxl::excel_sheets(filename)
  x <- lapply(sheets, function(X) readxl::read_excel(filename, sheet = X))
  if(!tibble) x <- lapply(x, as.data.frame)
  names(x) <- sheets
  x
}
cvdata <- read_all_excel_sheets(data_filepath)

```

# **Summary**

-   I am a technical orientated Fellow of the Institute of Actuaries working in the life insurance industry for `r {floor(interval(ISOdate(2012,9,3), now()) / years(1))}` years, with `r {floor(interval(ISOdate(2017,12,1), now()) / years(1))}` years post qualification. I also have experience as an R developer.

<!-- -->

-   I have a diverse range of actuarial work experience with my main areas of specialty being:

    -   Longevity

    -   Internal Model Risk Calibration

    -   Commercial Real Estate Loans

    -   With Profits Management

<!-- -->

-   I am technically strong and have a breadth of experience developing complex models. I am proficient across a variety of platforms (see @fig-software-experience).

<!-- -->

-   I have a fantastic track record of performance, both academically and in a work environment (see @fig-RL-Performance).

# **Qualifications**

```{r}
#| label: qual-hist
#| warning: false

qual.hist <-
  tribble(
    ~Qualification,  ~`Awarded By`, ~Grade,  ~`Subject(s)`,      ~Date,
    "Fellowship",    "IFoA", NA,    "SA2, ST2, ST6",    "2017-12-01",
    "Bachelors","Univ. of Cambridge", "1", "Natural Sciences (Chemistry)", "2011-07-01",
    "Masters", "Univ. of Cambridge", "2.i", "Natural Sciences (Chemistry)", "2011-07-01",
    "A Level",       NA, "A (x5)", "Maths, Further Maths, ...", "2007-08-01",
    "GCSE",          NA, "A* (x8); A (x3)", "Maths, English, ...", "2005-08-01"
  ) %>% 
  mutate(Date = as.Date(Date) %>% format('%b-%Y'))

qual.hist %>% 
  gt() %>% 
  tab_options(
        column_labels.background.color = "#008CBA",
        quarto.disable_processing = TRUE
        ) |>  
 tab_style_body(
    style = list(
      cell_fill(color = "grey95"),
      cell_text(color = "grey95")
      ),
      fn = \(x) is.na(x)
  )
```

Other achievements include:

-   One of top 5 scores in the UK for GCSE Maths (awarded by AQA in Aug 2005)

-   IFoA Certificate in Data Science (awarded by IFoA in Dec 2020)

# **Software Experience**

The plot below demonstrates my career experience, both total accumulation and the relative recency. The figures are estimates as at 31/12/2024.

Please note the scale is logarithmic.

```{r}
#| label: fig-software-experience 
#| fig-cap: "Summary of Modelling Experience" 
#| cap-location: top 
#|  

experience <- cvdata$Software %>% rename_with(tolower)

exp_long <- experience %>%
  gather(key = year, value = hours, -"platform") %>%
  mutate(
    year = as.numeric(year),
    hours = as.numeric(hours)
  ) %>% 
    filter(!is.na(hours)) 

# get order of platforms in terms of total experience. These are platform_levels and used to order platforms within plot at the end
platform_levels <- exp_long %>%
  group_by(platform) %>%
  summarise(totes = sum(hours, na.rm = T)) %>%
  arrange(desc(totes)) %>%
  .$platform %>%
  rev()

years <- exp_long$year %>%  unique() %>% as.numeric() %>% sort()

# need to calculate marginal log hours for plot. Simple log transform of values will not work because log(sum(hours)) != sum(log(hours))
# NOTE we are arranging by highest year first when getting cumulative values... this is design choice, as most recent years are most relevant and so should look bigger
exp_plot_data <-
exp_long %>% 
  arrange(platform, desc(year)) %>% 
  group_by(platform) %>% 
  mutate(
    cum_hours = cumsum(hours),
    cum_log_hours = ifelse(cum_hours == 0, 0, log(cum_hours)),
    marginal_log_hours = diff(c(0, cum_log_hours), 1),
    platform = factor(platform, levels = platform_levels)
  ) 

#check no negative values
if(min(exp_plot_data$hours)<0) {stop("graph will distort because log hours include negative values")}

# set up x axis parameters
x.max <- exp_plot_data$cum_log_hours %>% max()
x.min <- 10 %>% log()
x.breaks <- c(10,100,1000,10000) %>% log()
x.labels <- exp(x.breaks)

# set up mid-year point for colour scale
mid.year <-  min(years) + 0.5*(max(years) - min(years)) 


# plot of software experience (final output)
software_experience_plot <-
ggplot(exp_plot_data, aes(
  x = marginal_log_hours, 
  y = platform,
  fill = year,
  label = hours
)) +
  geom_col(width=0.62) +
  scale_fill_gradient2(
    name = "Year",
    breaks = scales::pretty_breaks(),
    low = "grey97",
    mid = "steelblue1",
    midpoint = mid.year,
    high = "steelblue4"
  ) +
  coord_cartesian(xlim = c(x.min, x.max)) +
  scale_x_continuous(
    name = "Estimated Cumulative Experience (Hours)",
    breaks = x.breaks,
    minor_breaks = NULL,
    labels = scales::comma(x.labels)
  ) +
  labs(caption = "**please note the scale is logarithmic") +
  ylab("Platform") +
  theme_classic() + 
  theme(panel.grid.major.x = element_line()) 

software_experience_plot %>% ggplotly(tooltip = c("y", "fill", "label")) 


```

# **Performance**

Whilst employed by Royal London during years 2013 - 2020 I was assigned a performance rating each year, as were all staff. Employees were placed in one of four bands which represent different percentile ranges of performance relative to colleagues. Those bands, and the amount of times I was placed in each band over the 8 year period, are: Improvement Required (0); Good (3); Strong (4) and Exceptional (1). 

Below I have mapped my historical ratings to a probability density of performance across all percentiles. The density is calculated by the relative frequency of each rating awarded to me, divided over the percentiles of employee performance that rating represents. I have also included the probability density you would expect from an actuary selected randomly from Royal London, for comparison purposes.

```{r}
#| label: fig-RL-Performance
#| fig-cap: "Performance rating density whilst at Royal London"


actuary.levels <- c("Random Actuary", "Paul Beard")
grade.levels <- cvdata$`RL Performance`$grade

data <- cvdata$`RL Performance` %>%
  mutate("Paul Beard" = count / (maxpercentile - minpercentile) / sum(count),
         "Random Actuary" = 1) %>%
  pivot_longer(cols = c("Paul Beard", "Random Actuary"), names_to = "actuary", values_to = "density") %>%
  select(grade, minpercentile, maxpercentile, density, actuary, count) %>% 
  mutate(count = if_else(actuary == "Random Actuary", NA, count))

data$grade <- factor(data$grade, levels = grade.levels)
data$actuary <- factor(data$actuary, levels = actuary.levels)
performance_rating_plot <-
  ggplot(data, aes(
    ymin = 0,
    fill = grade,
    label = count
  )) +
  geom_rect(
    aes(
      xmin = minpercentile,
      xmax = maxpercentile,
      ymax = density
    ),
    colour = "white"
  ) +
  theme_bw() +
  scale_x_continuous(breaks = c(seq(0, 1, 0.2)), labels = scales::percent(seq(0, 1, 0.2))) +
  scale_y_continuous(breaks = c(seq(0, 3, 0.5))) +
  theme(
    legend.position = "bottom",
    strip.background = element_rect(fill = "light grey")
  ) +
  scale_fill_manual(name = "Performance Rating", values = c(
    "lightblue1",
    "steelblue1",
    "steelblue3",
    "steelblue4"
  )) +
  labs(
    x = "Percentile of Performance",
    y = "Probability density"
  ) +
  facet_wrap(~actuary) +
  theme(legend.position = "none")

performance_rating_plot %>% ggplotly()
```

::: callout-tip
Choosing Paul Beard is a superior strategy to choosing an actuary at random.
:::

# **Work Experience**

### **SCOR**

*Employed as Contractor (Feb 2024 — Feb 2025)*

#### R Developer for Longevity Pricing (Feb 2024 — Feb 2025)

-   The initial project was completed for UK reinsurance pricing of BPA schemes. The project entailed migrating the entire pricing process from Excel / SAS / Word, to a single R workflow. I was the lead developer of an R package and Quarto book that together could perform all pricing calculations required.

-   The migration included building the many complex pricing models in R, some of these include:

    -   Pension Value Escalation model

    -   Proportion Married & Proportion Transferred Out derivation models

    -   Experience Analysis model

    -   Actual vs Expected Analysis and Adjustment model

    -   Net Present Value Cashflow model

    -   Risk Based Cashflow (i.e. Stressed Capital Requirement) models

-   As well as being able to develop complex actuarial models, the project entailed the following:

    -   High quality visualisations including interactivity

    -   Extensive reconciliation and testing

    -   Developing functions that are flexible, fast and documented via Roxygen

    -   Working with Git, Azure and Quarto

    -   Development of code to interact directly with an API of Club Vita

    -   Consideration of usability, to develop a template that other actuaries, with less experience in R, could employ

-   The initial contract was was for UK business and scheduled over 6 months. The project was completed successfully and to a high standard. The client, pleased with the product, decided to extend the contract for another 6 months to develop similar processes for non-UK business .

### **Phoenix Group**

*Employed as Contractor (Apr 2023 — Feb 2024)*

#### Assumptions Architect for BPAs (April 2023 — Feb 2024)

-   Phoenix uses MG-Alfa as their main valuation platform. The tables fed into MG-Alfa were created through a set of complex spreadsheets. In this role I developed these spreadsheets in co-ordination with the release cycle.

-   Whilst at Phoenix I created a sophisticated comparison tool in R which could compare two complex assumptions spreadsheets. By being able to replicate the tenets of how MG-Alfa reads table structures within R, the tool could produce an intelligent breakdown of assumption table changes which was superior to alternatives being used at the time (e.g. VBA cell-by-cell comparison, BeyondCompare). The tool was packaged and rolled out to other actuaries.

### **Royal London**

*Employed as Full-time Employee (Sep 2012 — Apr 2023)*

#### Credit Quality Assessor of Commercial Real Estate Loans (CRELs) (Aug 2021 — Apr 2023)

-   Royal London looked to expand the illiquid asset classes it invests in as part of its Matching Adjustment portfolio. This entailed the inclusion of CRELs. In this role I developed an internal credit quality assessment process (ICQA) for CRELs. As this asset class was new to the company, it required a lot of development from ground level.

-   As part of the role I created a complex stochastic model in R which captures the intricate cash flow structure of CRELs. The model simulates defaults across multiple correlated tenants over many time periods, and evaluates the outcomes of the loans to assess the credit quality. 

-   After developing the ICQA process for CRELs, I then developed the internal model risk calibration for CREL risk.

#### Actuarial Student Overseer (Jun 2018 — Nov 2021)

-   Volunteered as coordinator and adviser to over 60 students sitting exams at Royal London. This role has required strong communication and leadership skills. I made sure to always be approachable to the students, and keep open paths of dialogue. 

#### Internal Model Calibrator (Mar 2018 — Aug 2021)

-   I worked on the development of the company’s Internal Model Application Process (IMAP) and produced calibrations for Longevity, Persistency, Commodity, Property, Mortality/Morbidity risks, as well as Market-to-market Dependencies.

-   Answered questions from both internal validators and the PRA about our IMAP methodology in a short period of time. Resolved validation findings in a timely manner.

-   I have led many workshops to co-ordinate the opinions of experts in their respective fields, and form consensus on the assumptions and methodology used in the internal model calibrations.

#### With Profits Management Actuary (Sep 2012 — Mar 2018)

-   Within this role I carried out regular analysis on bonus supportability for With Profits business and became a subject matter expert and led a team of actuarial staff to complete year end final bonus reports.

-   I was involved in many with profits management tasks including (but not limited to) bonus setting, projections, run-off planning, estate distribution and remediations.

-   I have been involved in many bespoke projects, the most material being:

    -   Guaranteed Annuity Option (GAO) compromise scheme pricing

    -   Surrender value methodology development

    -   Prophet to MG-Alfa model reconciliation

# **Other Projects**

I write and publish articles applying actuarial tools to publicly available data. These include:

-   A mortality investigation of Coronation Street

-   An internal model risk calibration for cryptocurrency risk

-   An analysis of trends in IFoA exam pass rates

The articles can be found below, along with the R code used to produce the analyses.

Articles: <https://www.linkedin.com/in/paul-beard-78a420172/recent-activity/posts/> 

Code: <https://github.com/paulbeardactuarial> 