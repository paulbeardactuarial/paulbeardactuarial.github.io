[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "About\n\n\n\nMy name is Paul Beard. I am a technical orientated Fellow of the Institute of Actuaries working in the life insurance industry for over 12 years, with over 8 of those years being post qualification.\nI am also experienced as a programmer and have built complex models, complete packages and everything on this website!\nThe combination of these skills allows me to build end-to-end actuarial workflows. If you would like to upgrade your actuarial systems, please get in touch to find out what is possible."
  },
  {
    "objectID": "visuals.html",
    "href": "visuals.html",
    "title": "Visualisations Gallery",
    "section": "",
    "text": "The following is a sample of visualisations produced using “Tidy Tuesday” data sets:\n\n\n\nPalm Trees: Comparison of Species’ Heights\n\n\n\n\n\n\n\nPixar: Film Ratings of Sequels\n\n\n\n\n\n\n\nLong Beach Animals: Cat Birthday Distributions"
  },
  {
    "objectID": "crypto_calibration.html",
    "href": "crypto_calibration.html",
    "title": "Crypto Risk Calibration",
    "section": "",
    "text": "Cryptocurrency first manifested into existence in 2008, when the first ever Bitcoin was mined. It has since grown, with thousands of variants now available to purchase by the armchair investor. Some financial institutions have also gotten in on the game, although the level of trading pales compared to other major asset classes.\nBy applying principles used by insurance companies in asset risk calibration, I have constructed a risk calibration for cryptocurrency risk. I am not aware of any insurers who would need such a calibration, but if they did, they might take inspiration from this article.\nDisclaimer: This article is NOT an endorsement of cryptocurrency as an asset class."
  },
  {
    "objectID": "crypto_calibration.html#the-premise",
    "href": "crypto_calibration.html#the-premise",
    "title": "Crypto Risk Calibration",
    "section": "The Premise",
    "text": "The Premise\nAn insurance company has a non-trivial holding in the cryptocurrency markets. An internal model calibration for this risk exposure is required. By applying some of the principles used for other major asset classes, such as property and equity, I consider what that risk calibration might look like.\nIt is also expected that the insurance company would have a significant holding in UK equity markets, and that the insurer has a risk scenario generator. These assumptions are necessary when considering how our cryptocurrency risk might contribute to the final SCR of a company."
  },
  {
    "objectID": "crypto_calibration.html#data",
    "href": "crypto_calibration.html#data",
    "title": "Crypto Risk Calibration",
    "section": "Data",
    "text": "Data\n\nEmpirical Data\nIndex data was taken from a publicly available source: https://www.investing.com/. We have selected the data for value in GBP of the four of the largest cryptocurrencies by market capitalisation: Bitcoin (BTC), Ethereum (ETH), Ripple (XRP), and Dogecoin (DGE).\nWe have used data as far back as available, capped to 2012. This represents different start dates for different cryptocurrencies. We have:\n\nJan-12 for BTC (Bitcoin)\nFeb-15 for XRP (Ripple)\nApr-16 for ETH (Ethereum)\nJul-17 for DGE (Dogecoin)\n\nThe data used is up until May-25, which was the latest available month at time of writing.\nWe performed an array of stationarity tests on the Bitcoin dataset. In all cases p-values were above 0.05, meaning we could not reject any of the null hypotheses. This was both in tests where the null hypothesis was the data contains a trend, and when the null hypothesis was the data is level.\nThe index data looks as follows:\n\n\n\n\n\n\n\n\nTransformation(s)\nIndex data is transformed to monthly annual overlapping investment returns. The benefit of this is the generation of extra data points. Given the limited history of cryptocurrency, this was an essential transformation. The downside to this approach would be any autocorrelation within our datasets.\nWe are completing our analysis on the excess-of-mean returns. To do this, we deducted the historical mean from our datasets, such that the mean of the simple returns is 0%. It should be noted that this was not a trivial amount for cryptocurrency.\nIn this analysis we have chosen to use log returns. This has the advantage of allowing returns to occupy a range between ±∞, unlike simple returns which can theoretically be below -100%. This is particularly useful for the incredibly volatile asset that is crypto-currency, where distribution fitting would likely extend beyond the -100% for non-trivial probabilities.\nWe then added in a risk premium to reflect the best estimate of expected returns for this asset class. When choosing a value it is worth considering what type of asset cryptocurrency is. If it is indeed a currency, then a risk premium of 0% would seem appropriate given currency is not a growth asset. However cryptocurrency appears to act more like a commodity that investors purchase in the hope that someone will pay more for it in the future. This is more akin to a bar of gold than a dollar.\nGiven the asset acts more like a commodity, we are assuming the expected return to be similar to expected inflation. At time of writing (May 2025) the inflation rate is high, however the Office for Budget Responsibility (OBR) has predicted from 2026 rates will be around 2%, with a range of around 0-5%. We have therefore set our risk premium to be 3%, closer to the current rate of inflation. [https://obr.uk/efo/economic-and-fiscal-outlook-march-2025]\nIt should be noted that when we combined these transformations, the mean of our datasets was very negative. For example the mean of log transformed BTC data, before adding any risk premium adjustments, was -81%. This is due to the extreme volatility found in cryptocurrency, as well as the large returns in general.\n\n\nAnnualised Change\nWe present the empirical data for our 4 chosen cryptocurrencies in the plots below. Firstly we show the time series of the annual log returns, as well as histograms of their distributions.\n\n\n\n\n\n\nPerhaps the most striking things from the data are:\n\nThe annualised log changes are enormous. For comparison a similar distribution for UK Equities would be very rare to see a point beyond the ±50% range.\nThe 4 different cryptocurrencies are highly correlated in their movements. They almost move in complete synchronicity.\nThe distributions have quite negative means. This is a consequence of our log transformations. This amount can often be trivial for standard assets, however as the variance of cryptocurrency returns is huge, the mean of log returns is a lot lower than 0%."
  },
  {
    "objectID": "crypto_calibration.html#distribution-fitting",
    "href": "crypto_calibration.html#distribution-fitting",
    "title": "Crypto Risk Calibration",
    "section": "Distribution Fitting",
    "text": "Distribution Fitting\nWe tried to fit a range of distributions to each one of our data sets. The distributions attempted (with the number of parameters in parentheses) are:\n\nNormal (2)\nLogistic (2)\nStudent T (3)\nHyperbolic (4)\n\nWe show the QQ plots for these fitting attempts below. We decided not to show the Student T as it typically converged to the Normal distribution and did not offer much alternative in terms of distribution choice.\n\n\n\n\n\n\n\n\n\nFrom the QQ plot we can infer:\n\nFor BTC, ETH and XRP it seems like the Hyperbolic distribution fits the best. This is particularly true around the negative tail, which is the most important part of the distribution for our purposes.\nFor DGE the Hyperbolic also seems to fit better around the negative tail. It should be noted that the unusual history of Dogecoin was hard to fit any distribution to. This is perhaps not surprising for an asset that is influenced more by Elon Musk tweets than any other market force.\nThe Normal, Student T and Logistic distributions struggled around the tail. This is perhaps not surprising given all 3 are symmetrical distributions, and the actual historical returns are highly skewed.\n\nAlso helpful in determining the best fit are fitting statistics. We have calculated the following values:\n\nResidual Sum of Squares (RSS) - this is the sum of least square values from our QQ plots. The lower the value, the better the fit.\nMaximum Likelihood Estimator (MLE) - this is the likelihood of our empirical points upon our fitted distribution. The higher the value, the better the fit.\nAkaike Information Criterion (AIC) - this is a fitting statistic based on the MLE. The lower the value, the better the fit.\nBayes Information Criterion (BIC) - similar to AIC, this statistic also penalises higher distributions with more parameters. The lower the value, the better the fit.\n\nWe present these in the tables below. To make deciphering easier, we have applied a colour scale over the table with redder values being poorer and whiter values being better fitting statistics:\n\nBTCETHXRPDGE\n\n\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n  \n  \n    \n      Distribution\n      RSS\n      LogLik\n      AIC\n      BIC\n    \n  \n  \n    Normal\n2.371\n−230.8\n465.6\n471.6\n    Logistic\n2.531\n−229.7\n463.3\n469.3\n    Student T\n2.481\n−229.8\n465.7\n474.7\n    Hyperbolic\n1.451\n−226.9\n461.8\n473.8\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n  \n  \n    \n      Distribution\n      RSS\n      LogLik\n      AIC\n      BIC\n    \n  \n  \n    Normal\n3.014\n−174.7\n353.4\n358.6\n    Logistic\n3.425\n−174.9\n353.8\n358.9\n    Student T\n3.026\n−174.7\n355.3\n363.1\n    Hyperbolic\n2.281\n−169.6\n347.3\n357.6\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n  \n  \n    \n      Distribution\n      RSS\n      LogLik\n      AIC\n      BIC\n    \n  \n  \n    Normal\n5.674\n−197.5\n399.0\n404.4\n    Logistic\n5.865\n−190.5\n384.9\n390.4\n    Student T\n7.888\n−185.9\n377.7\n385.9\n    Hyperbolic\n2.073\n−173.7\n355.3\n366.2\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n  \n  \n    \n      Distribution\n      RSS\n      LogLik\n      AIC\n      BIC\n    \n  \n  \n    Normal\n6.151\n−157.6\n319.2\n324.1\n    Logistic\n6.558\n−153.5\n310.9\n315.8\n    Student T\n12.299\n−146.9\n299.8\n307.1\n    Hyperbolic\n3.826\n−138.7\n285.5\n295.1\n  \n  \n  \n\n\n\n\n\n\n\nIt is clear for all 4 crypto assets, the Hyperbolic distribution has the better fitting statistics. This is true across the RSS, LogLik, AIC and BIC. The only occasion where Hyperbolic does not give the best result is for the BIC for Bitcoin. The BIC applies larger penalties for higher numbers of parameters, and as Hyperbolic has the most (at 4 parameters), it seems this is enough penalty to demote the Hyperbolic distribution. Despite this, it is quite clear Hyperbolic is preferable for every crypto asset analysed."
  },
  {
    "objectID": "crypto_calibration.html#final-distributions",
    "href": "crypto_calibration.html#final-distributions",
    "title": "Crypto Risk Calibration",
    "section": "Final Distributions",
    "text": "Final Distributions\nOur final chosen fitted distributions are:\n\nBTC - Hyperbolic\nETH - Hyperbolic\nXRP - Hyperbolic\nDGE - Hyperbolic\n\nBelow we show the key percentiles for change in asset value over a one year time horizon. To make the results easier to interpret, we have converted the annualised log returns back into annualised simple returns.\n\n\n\n\n\n  \n    \n      Percentile\n      BTC\n      ETH\n      XRP\n      DGE\n    \n  \n  \n    99.5%\n+1,442.3%\n+7,974.4%\n+5,147.9%\n+8,516.5%\n    97.5%\n+457.9%\n+1,036.9%\n+560.1%\n+815.4%\n    90.0%\n+104.7%\n+103.4%\n+9.7%\n+27.6%\n    50.0%\n−58.5%\n−76.9%\n−87.1%\n−89.1%\n    10.0%\n−88.4%\n−93.9%\n−95.2%\n−96.7%\n    2.5%\n−93.4%\n−96.9%\n−96.7%\n−97.6%\n    0.5%\n−95.9%\n−98.5%\n−97.8%\n−98.0%\n  \n  \n  \n\n\n\n\nIt is quite clear that a 1-in-200 event would essentially wipe out all cryptocurrency value. In fact, even a 1-in-10 event would cause significant damage to asset values.\nEven the median of these distributions is very negative, with a 1-in-2 event seeing Bitcoin values almost half over the year. The reason we observe this is a consequence of using log transformed returns to fit our distributions to. This was explained at the end of the ‘Data Transformation’ section.\n\nSpecific Risk\nAnother consideration to make would be to have an overlay for this risk distribution. This would be with the aim of recognizing that we have fitted our distribution to a fairly short time period of of around 8-13 years, which could mean there are no extreme tail events within the dataset.\nThe risks to any single currency are not insignificant. https://www.hedgewithcrypto.com/ notes that there have been between 1,700 and 2,500 cryptocurrencies to collapse into zero value. The largest being ‘Luna’ which had been a top 10 crypto by market capitalisation before collapsing to zero.\nThis information may be a reason to apply an overlay to our current distributions. We have not explored this any further here. This specific risk could potentially be diversified away through investment in many different crypto assets."
  },
  {
    "objectID": "crypto_calibration.html#correlations",
    "href": "crypto_calibration.html#correlations",
    "title": "Crypto Risk Calibration",
    "section": "Correlations",
    "text": "Correlations\nWe consider the correlations of market movements between cryptocurrencies, as well as against other asset classes. By calculating Spearman’s rank correlation, we construct a correlation matrix, which is visualised below:\n\n\n\n\n\n\n\nCrypto Intra-dependencies\nBy diversifying our portfolio we might remove specific risks (i.e. the risk of an individual crypto collapsing, like Luna). However systemic risk may still exist across all cryptocurrencies.\nCryptocurrencies are very highly correlated in their market movements. There is barely any systemic diversification benefit between the currencies. Given the very high correlation already in our historical data, we would suggest using the same risk driver to model all cryptocurrency risk in a portfolio would likely be sufficient.\n\n\nCrypto Inter-dependencies\nHow does cryptocurrency correlate with other asset classes? Bitcoin is sometimes described as “Digital Gold” [https://www.db.com/what-next/digital-disruption/dossier-payments/i-could-potentially-see-bitcoin-to-become-the-21st-century-gold]. Earlier we likened the two assets when trying to assign a risk premium for our crypto coins. Gold is a commodity. It is also negatively correlated to market trends in major asset classes such as equities. When the market takes a downturn, gold can often rise as investors see it to be a safe haven. As such gold can provide utility as a partial hedge within an investment portfolio.\nBased on the correlations of cryptocurrency against Gold Futures and FTSE 100, we draw the following conclusions:\n\nGold is (weakly) negatively correlated to the UK Equity market, as shown by the correlation of -0.15. It could be considered as a good asset for diversification in an equity rich portfolio.\nCryptocurrency acts nothing like Gold. The correlation is close to zero for all cryptocurrencies tested.\nCryptocurrency is positively correlated to the FTSE 100, and not by a neglible amount. BTC scores a correlation on +0.50 against the FTSE 100. It appears to offer little in the way of hedging to the UK Equity market. If we take the price of gold to reflect (lack of) confidence in the market, it seems Bitcoin does not copy this reflection at all.\nWith material positive correlation to UK Equity market, and the extreme distributions we have fitted to cryptocurrency data, it is likely most insurance companies would observe an obliteration of crypto values within their biting scenario (if they held crypto)."
  },
  {
    "objectID": "crypto_calibration.html#conclusion",
    "href": "crypto_calibration.html#conclusion",
    "title": "Crypto Risk Calibration",
    "section": "Conclusion",
    "text": "Conclusion\nThe following conclusions can be made:\n\nCryptocurrencies are incredibly volatile.\nApplying our techniques lead to a distribution where only a 1-in-10 event would see almost all asset value wiped out. Even a median event will see asset values decline sharply.\nCryptocurrencies are highly correlated between each other in market movements. There is no doubt modeling all crypto assets with the same risk driver would be sufficient.\nCrypto assets are positively correlated with the UK equity market. Given equity risk is often a significant risk for a major insurer, we expect many SCR biting scenarios to be at the negative end of the distribution for equities. Given this correlation, the crypto biting scenario will also likely be at the negative end of the distribution.\nGiven all these points above, we would expect the overall biting scenario for a major insurer to see cryptocurrency value completely decimated. The conclusion reached is that almost all assets held as cryptocurrency will increase the SCR of a company by a similar amount. This conclusion might lead an actuary to question how capital efficient it is to invest in crypto.\n\n\nThe code used to generate the results in this article can be found at: https://github.com/paulbeardactuarial/crypto_calibration\n\n\nThis article was written by Paul Beard and published on 30th May 2025"
  },
  {
    "objectID": "artificial_actuary_cod.html",
    "href": "artificial_actuary_cod.html",
    "title": "Artificial Actuary: Classifying Causes of Death",
    "section": "",
    "text": "Recently an actuarial colleague came across a problem. They wanted to categorise a dataset containing many irregular and free-text causes of death, into broader categories. This can be a problem for any actuary working in longevity, where we might want to apply stresses for the various drivers of mortality to a select subgroup of the population.\nBefore the AI overlords take over, and the only cause of mortality is simply “death by robot”, I wanted to see if LLMs could help with this classification problem."
  },
  {
    "objectID": "artificial_actuary_cod.html#the-aim",
    "href": "artificial_actuary_cod.html#the-aim",
    "title": "Artificial Actuary: Classifying Causes of Death",
    "section": "The Aim",
    "text": "The Aim\nUsing R, connect to the API of various LLMs and get them to perform the onerous task of classifying data for us. Can we get the LLMs to do a job that might have taken an actuary lots of time and effort? Can we do it in a way that is automatable, such that if circumstances change we simply press a button and update?"
  },
  {
    "objectID": "artificial_actuary_cod.html#the-premise",
    "href": "artificial_actuary_cod.html#the-premise",
    "title": "Artificial Actuary: Classifying Causes of Death",
    "section": "The Premise",
    "text": "The Premise\nFor this experiment I will get different LLMs to classify the ICD-10 causes of death into broader categories. The categories I have pulled from a paper online related to the cause-specific mortality impact of smoking. The paper and categories are not too important, they are just an example of what an actuary might be use. The following paper…\nCigarette Smoking Cessation, Total and Cause-specific Mortality: A 22-Year Follow-up Study in US Male Physicians\n…states that smoking showed an increase in mortality for the following categories of disease:\n\ncoronary heart disease\ncerebrovascular disease\npulmonary disease\nlung cancer\ncolorectal cancer\nlarynx cancer\nkidney cancer\nacute myeloid leukemia\noral cavity cancer\nesophageal cancer\npancreatic cancer\nbladder cancer\nstomach cancer\nprostate cancer\nsudden death\n\nThis paper would be very useful, but my other dataset is the ONS cause of death mortality by ICD-10 code in England and Wales. A snapshot of just some of those causes of death is shown below:\n\n\n\n\nTable 1: Random Sample of Causes of Death\n  \n    \n      Cause of Death\n      No of Deaths 2013 to 2023\n    \n  \n  \n    Cushing syndrome unspecified\n42\n    Benign neoplasm Appendix\n2\n    Other specified diseases of biliary tract\n34\n    Sarcoidosis of lung\n602\n    Primary pulmonary hypertension\n570\n    Malignant neoplasm Glans penis\n17\n    Other instability of joint\n1\n    Malignant neoplasm External upper lip\n10\n    Somatoform autonomic dysfunction\n2\n    Occupant [any] of heavy transport vehicle injured in other specified transport accidents\n1\n  \n  \n  \n\n\n\n\nThere are thousands listed. Each category in our smoking paper can cover many causes of death in the data set. Can an LLM figure which ones out so an actuary won’t have to?"
  },
  {
    "objectID": "artificial_actuary_cod.html#method",
    "href": "artificial_actuary_cod.html#method",
    "title": "Artificial Actuary: Classifying Causes of Death",
    "section": "Method",
    "text": "Method\nMy initial attempt at this tried to use the {mall} package and an LLM that I downloaded straight to my computer. It became apparent quite soon that the processing power of LLMs is enormous, and too much for my humble laptop.\nInstead, I have used the brilliant {ellmer} package. This package allows R users to connect to a whole range of APIs available. Spoilt for choice, I set up a few links. The following LLMs were used:\n\nChat GPT 4o (gpt-4o-2024-08-06)\nChat GPT 4o Mini (gpt-4o-mini-2024-07-18)\nGemini (gemini-2.0-flash-001)\nLlama (llama-3.3-70b-versatile)\nDeepseek R1, distilled (deepseek-r1-distill-llama-70b)\n\nInitial attempts revealed some of the problems with using LLMs to clean data.\nThe first issue was around token limits and speed. Sending a single cause of death at a time would be unlikely to trigger the limits of the API and has the benefit of data structure being more regular (see second issue). However it would take far too long, and would be inefficient with token use asking the same question for each single cause of death. On the opposite end of the spectrum, sending data in larger chunks can sometimes cause failure as token limits are reached for a request. They also create data structure problem as the LLM is now giving us multiple data points in one chat we must decode this and hope they haven’t gone off-the-wall. In the end the best solution was to chunk the data and send over in batches.\nThe second main issue was getting data back in a regular format. It was almost comical how badly behave some of the LLMs were at being regular. Sometimes the no. of results didn’t match the no. of items sent. It was also impossible to get Deepseek to stop explaining itself in the output - no matter what I told it in the prompt! In the end it became apparent the best strategy was to communicate in JSON format and get the key results wrapped up in markers so I can extract it from the text and convert back to a dataframe using jsonlite::fromJSON(). The initial prompt evolved over time. Below is what I sent in relation to structure:\n\n“You are a classification LLM. You will receive a JSON file. The file will contain a list of items with cause_of_death. It is important that you return only an edited version of the JSON file. Add ‘category’ to each item, which can only ever pick one of the values below. No explanations. Return only the data in a structured JSON format. Your final JSON code must begin with ``` and end with ```”\n\nThe final issue was ambiguity. Some of the categories in the smoking paper could be interpreted in different ways. For example “pulmonary disease” technically means any disease of the lungs, yet this could entail something completely disconnected to smoking, such as tuberculosis with HIV or asbestos poisoning. Genetic disorders are also not going to be relevant to our risk driver in this case. In the end, I added further instruction to my LLM prompt:\n\n“If a cause of death cannot be linked to smoking in any way, for example if it is an infectious disease, a genetic disorder, or has an external cause provided in the cause_of_death text (e.g. asbestos), then assign the category as”none””\n\nThis didn’t eliminate some ambiguity over some of the groups. In particular, the terms “sudden death” and “coronary heart disease” weren’t clear to me what they entailed precisely. Given I was hoping to mark the results, I dropped “sudden death” and changed to “ischaemic heart disease” respectively.\nFinally, before I fed the causes of death into the LLM, I shuffled them. My original dataset was already ordered in a way that hinted at its groups. This was very handy for me when I had an attempt at categorising, but why should I give the robots the same privilege? By shuffling we prevent them getting wise to patterns, and demonstrate extra capability of the artificial actuary over the human actuary."
  },
  {
    "objectID": "artificial_actuary_cod.html#results",
    "href": "artificial_actuary_cod.html#results",
    "title": "Artificial Actuary: Classifying Causes of Death",
    "section": "Results",
    "text": "Results\nThere were 3,598 causes of death. I processed the category returned by each model, and compared to my own evaluation. A huge caveat here is that I am not a medical doctor, and have done the best with my very limited faculties. In general I have used the broader categorisation of ICD-10 codes that already exists.\nThe accuracy of every model was at least 93%. However because 3,353 of the causes of death fed in should not have been assigned a category, a score of 93% would also be attained by simply stating “none” for everything.\n\n\n\n\nTable 2: Accuracy and Completion Rates of LLMs\n  \n    \n    \n    \n  \n  \n    \n      Model\n      Accuracy\n      Proportion Answered\n    \n  \n  \n    unanimous_consensus\n99.6%\n88%\n    consensus\n97.6%\n100%\n    gpt_4o\n97.0%\n100%\n    deepseek_r1\n96.8%\n99%\n    gpt_4o_mini\n96.3%\n100%\n    gemini\n95.4%\n100%\n    llama_33\n93.3%\n100%\n  \n  \n  \n\n\n\n\nTo get a better understanding of the model success, we look at the stats of precision, recall and f1. A higher precision score means the model was often right when assigning a category (other than “none”). A higher recall score means the model did not often assign a category incorrectly (other than “none”). The F1 score combines these two to give an overall performance rating.\n\n\n\n\nTable 3: Performance Evaluation of LLMs\n  \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Model\n      \n        Count\n      \n      \n        Scores\n      \n    \n    \n      True Negative\n      False Negative\n      True Positive\n      False Positive\n      Precision\n      Recall\n      F1\n    \n  \n  \n    gpt_4o\n3282\n36\n190\n71\n73%\n84%\n78%\n    deepseek_r1\n3268\n53\n172\n62\n74%\n76%\n75%\n    gemini\n3219\n29\n197\n134\n60%\n87%\n71%\n    gpt_4o_mini\n3323\n103\n123\n29\n81%\n54%\n65%\n    llama_33\n3129\n18\n208\n222\n48%\n92%\n63%\n  \n  \n  \n\n\n\n\nWe can see that gpt_4o_mini is more reserved than the other models - only assigning a category to 132 causes of death. This results in the highest precision score, but a punishing recall score which leads it ranked near the bottom. Some of the mistakes made by gpt_4o_mini highlighted this over-cautiousness. For example the gpt_4o_mini model failed to class “intracerebral haemorrhage in brain stem” as a “cerebrovascular disease” which is probably one of the less ambiguous cases, and instead class it as “none”. There are many similar examples in the results data.\nWe can see that llama_33 had the opposite problem, assigning a category to 430 causes of death. This led to a great recall but poor precision, which ranked it last. Having looked through the individual results, it is clear to me that llama deserved its place at the bottom. It often assigned “{body part} cancer” to any disease with a corresponding body part. This led to many false positives which were fairly obvious. An example would be assigning “alcohol-induced acute pancreatitis” to “pancreatic cancer” which is clearly not appropriate given the prompt it was provided.\nGemini was also quite gung-ho, though faired better than llama.\nThe clear winners were gpt_4o and deepseek_r1. Looking through the differences, they were due to more ambiguous cases rather than clear error."
  },
  {
    "objectID": "artificial_actuary_cod.html#meta-results",
    "href": "artificial_actuary_cod.html#meta-results",
    "title": "Artificial Actuary: Classifying Causes of Death",
    "section": "Meta Results",
    "text": "Meta Results\nThe F1 scores attained were good, but not incredible. We are certainly not ready to hand the steering wheel over to our artificial friends just yet. But can we use the wisdom of crowds to boost our results further? I created two meta-models.\n“consensus” picks the most common category of the 5 models above\n“unanimous_consensus” picks the category that all 5 models agreed on, if they did not all agree it refrains from answering\nThe results are impressive, with “consensus” having strong scores, and “unanimous_consensus” having fantastic results. In all unanimous cases of differences, it was for the most ambiguous of categories, and I was no more sure of my own answer than the models. I could certainly have been persuaded that “unanimous_consensus” was the correct answer each time.\n\n\n\n\nTable 4: Performance Evaluation of Meta LLMs\n  \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Model\n      \n        Count\n      \n      \n        Scores\n      \n    \n    \n      True Negative\n      False Negative\n      True Positive\n      False Positive\n      Precision\n      Recall\n      F1\n    \n  \n  \n    unanimous_consensus\n3016\n10\n106\n3\n97%\n91%\n94%\n    consensus\n3303\n35\n191\n50\n79%\n85%\n82%\n  \n  \n  \n\n\n\n\nAlthough the pickiness of “unanimous_consensus” helped boost its scores, it came at a cost. This meta-model failed to provide a category for 444 causes of death, which was 12% of the data fed in. One could still say that 88% of our data was categorised with high accuracy is a great win, and the actuary now only has to deal with 444 cases instead of the overwhelming 3,500+.\nThe “consensus” meta-model actually returned an answer for every cause of death. There was never a split decision between the LLMs. This strategy would be a good way to go if we were adamant we didn’t want to review any of the data ourselves!"
  },
  {
    "objectID": "artificial_actuary_cod.html#conclusion",
    "href": "artificial_actuary_cod.html#conclusion",
    "title": "Artificial Actuary: Classifying Causes of Death",
    "section": "Conclusion",
    "text": "Conclusion\nAlthough many actuaries now ask questions of AI to help with their day-to-day work, using R it is possible to go further and integrate the AI into actuarial data processing and even models. I have provided just one use case here, but there are many more out there for the typical actuary.\nUsing one LLM is helpful. Using many is very helpful. We are not at a point where the artificial actuary can replace the human one yet. The LLMs cannot understand the broader context of the problem, and this was clear to me each time I had to alter the prompt to get things working.\nThis demonstration not only highlights the usefulness of AI, but the enhanced possibilities when working with the more modern actuarial coding languages like R and Python.\nIf you are interested in the complete results, they are collected in the interactive table below:\n\n\n\n\n\n\n \nThe code used to generate the results in this article can be found at: https://github.com/paulbeardactuarial/cause_of_death_with_LLM\n\n\nThis article was written by Paul Beard and first published on February 13, 2025"
  },
  {
    "objectID": "paul_beard_for_hire.html",
    "href": "paul_beard_for_hire.html",
    "title": "Paul Beard FIA",
    "section": "",
    "text": "I am a technically strong Fellow of the Institute of Actuaries working in the life insurance industry for 12 years, with 8 years post qualification.\nI am an experienced coder. I have built complex models, complete packages and everything on this website! I have mastery over Excel and R, but am also adept across other platforms (see Software Experience). I understand good data and programming practices.\nI have a fantastic track record of performance, both academically and in a work environment.\n\n\n\n\n\nQualifications\n\n\n\n\n\n  \n    \n      Qualification\n      Awarded By\n      Grade\n      Subject(s)\n      Date\n    \n  \n  \n    Fellowship\nIFoA\nNA\nSA2, ST2, ST6\nDec-2017\n    Masters\nUniv. of Cambridge\n2.i\nNatural Sciences (Chemistry)\nJul-2011\n    Bachelors\nUniv. of Cambridge\n1\nNatural Sciences (Chemistry)\nJul-2011\n    A Level\nNA\nA (x5)\nMaths, Further Maths, ...\nAug-2007\n    GCSE\nNA\nA* (x8); A (x3)\nMaths, English, ...\nAug-2005\n  \n  \n  \n\n\n\n\nOther achievements include:\n\nOne of top 5 scores in the UK for GCSE Maths (awarded by AQA in Aug 2005)\nIFoA Certificate in Data Science (awarded by IFoA in Dec 2020)\n\n\n\n\n\n\nSoftware Experience\nThe plot below demonstrates my career experience, both total accumulation and the relative recency. The figures are estimates as at 30/06/2025.\nPlease note the scale is logarithmic.\n\n\n\n\nFigure 1: Summary of Modelling Experience\n\n\n\n\n\n\n\n\n\n\n\n\n\nWork Experience\n\nChoose Your Own Adventure. What Work Experience Are You Interested In?\n\n\n\n\n\n\n\n\nSCOR (Employed as Contractor, Feb 2024 — Feb 2025)\n\nOverview\n\nIn my time at SCOR I was the lead R developer in a project to migrate the entire annuity pricing model suite from Excel / SAS / Word into to a single R workflow.\nSCOR were impressed with my work and offered an extension after 6 months.\nWhen the project was completed after 12 months, management stated it was the first ever IT-based project they had known at SCOR to finish on-time and on-budget.\n\n\n\nR Developer for Reinsurance Annuity Pricing Model Migration (Feb 2024 — Feb 2025)\n\n\n\nThe initial project was completed for UK reinsurance pricing of BPA schemes. The project migrated the convoluted pricing process into a slick single R workflow. We revolutionised the pricing process at SCOR. I was the lead developer of an R package and Quarto book that together could perform all pricing calculations required, and produce html reports.\n\n\n\n\nThe migration included building many complex pricing models in R:\n\nPension Value Escalation model\nProportion Married & Proportion Transferred Out derivation models\nExperience Analysis model\nActual vs Expected Analysis and Adjustment model\nNet Present Value Cashflow model\nRisk Based Cashflow (i.e. Stressed Capital Requirement) models\n\n\n\n\n\nAs well as being able to develop complex actuarial models, the project entailed the following:\n\nDeveloping functions that are flexible, fast and documented via Roxygen\nDevloping a complete package of functions, data and tests that was rolled out to other actuaries\nWorking with Git, Azure and Quarto\nDevelopment of code to interact directly with an API of Club Vita to download mortality curves\nConsideration of usability, to develop a template that other actuaries, with less experience in R, could employ\nHigh quality visualisations including interactivity\nExtensive reconciliation and testing\n\n\n\n\n\n\nPhoenix Group (Employed as Contractor, Apr 2023— Feb 2024)\n\nOverview\n\nIn my time at Phoenix I contributed to the assumptions structuring of the MG-Alfa capital model whilst working under the tight quarterly release cycle. I worked with incredibly complex spreadsheets that existed there, and often came up with innovative solutions to improve the neglected systems that were in place.\nPhoenix were impressed with my work and offered an extension after 6 months.\n\n\n\nAssumptions Architect for BPAs (April 2023 — Feb 2024)\n\n\n\nWhilst at Phoenix I noted that there was considerable manual burden on checking spreadsheet changes. I therefore took the initiative to create a sophisticated comparison tool in R which could compare two complex assumptions spreadsheets.\nBy being able to replicate the tenets of how MG-Alfa reads table structures within R, the tool could produce an intelligent breakdown of assumption table changes which was superior to alternatives being used at the time (e.g. VBA cell-by-cell comparison, BeyondCompare). The tool was packaged and rolled out to other actuaries.\n\n\n\n\n\nRoyal London (Employed as Permanent, Sep 2012— Apr 2023)\n\nOverview\n\nI worked at Royal London for over 10 years, gaining my fellowship as well as a vast array of experience across many actuarial sectors.\nDuring that time I built up a strong reputation as someone who was technically astute and dependable. During appraisals I was consistently rated in the higher categories.\n\n\n\nCredit Quality Assessor of Commercial Real Estate Loans (CRELs) (Aug 2021 — Apr 2023)\n\n\n\nRoyal London looked to expand the illiquid asset classes it invests in as part of its Matching Adjustment portfolio. This entailed the inclusion of CRELs. In this role I developed an internal credit quality assessment process (ICQA) for CRELs. As this asset class was new to the company, it required a lot of development from ground level.\n\n\n\n\nAs part of the role I created a complex stochastic model in R which captures the intricate cash flow structure of CRELs. The model simulates defaults across multiple correlated tenants over many time periods, and evaluates the outcomes of the loans to assess the credit quality. \n\n\n\n\nAfter developing the ICQA process for CRELs, I then developed the internal model risk calibration for CREL risk.\n\n\n\nActuarial Student Overseer (Jun 2018 — Nov 2021)\n\nVolunteered as coordinator and adviser to over 60 students sitting exams at Royal London. This role has required strong communication and leadership skills. I made sure to always be approachable to the students, and keep open paths of dialogue. This role was also an official volunteer position (Student Employer Contact) with the IFoA.\n\n\n\nInternal Model Calibrator (Mar 2018 — Aug 2021)\n\n\n\nI worked on the development of the company’s Internal Model Application Process (IMAP) and produced complete insurance risk calibrations for Longevity Trend, Longevity Level, Mortality, Morbidity and Mortality Catastrophe risks.\n\n\n\n\nI also produced IMAP calibrations for Persistency, Commodity and Property risk. I also completed calibration of Market-to-market Dependencies looking at correlations between major risk groups.\nAnswered questions from both internal validators and the PRA about our IMAP methodology in a short period of time. Resolved validation findings in a timely manner.\nI have led many workshops to co-ordinate the opinions of experts in their respective fields, and form consensus on the assumptions and methodology used in the internal model calibrations.\n\n\n\nWith Profits Management Actuary (Sep 2012 — Mar 2018)\n\n\n\nWithin this role I carried out regular analysis on bonus supportability for With Profits business. Royal London has a large portfolio of with profits business, often gained through acquisitions. Different business lines often has different methods/rules attached, giving me a broad and deep persepective on how with profits business can work.\n\n\n\n\nAfter many years working in with profits I became a subject matter expert, culminating in me leading a team of five junior actuarial staff to complete the year end final bonus reports.\n\n\n\n\nI was involved in many with profits management tasks including (but not limited to) bonus setting, projections, run-off planning and estate distribution.\nI have been involved in many bespoke projects, the most material being:\n\nSurrender value methodology development\nProphet to MG-Alfa model reconciliation\nPolicyholder remediation calculations\n\n\n\n\n\nI was a major contributor to Royal London’s Guaranteed Annuity Option (GAO) compromise scheme. In this scheme, Royal London bought GAOs from policyholders in return for a material uplift to their asset share. My role was calculating a fair price for Royal London to offer the policyholders. This offer was submitted to the court and approved as a mass buyback, where policyholders were opted in by default but could choose to opt out.\n\n\n\n\n\n\n\n\nOther Projects\nI write actuarial-based articles on my LinkedIn profile. I am currently in the process of migrating these to my website. Some highlights include:\n\nArtificial Actuary: Classifying Causes of Death An article that leverages the use of LLMs via an API connection to cleanse / categorise cause of death data.\nCoronation Street: The Most Dangerous Street in the UK? An article that tries to make sense of the mortality rates experienced on the TV show ‘Coronation Street’.\nCryptocurrency: An Internal Model Risk Calibration An article that demonstrates what an Internal Model Risk calibration might look like for a hypothetical insurer invested in non-trivial amounts of cryptocurrency assets.\n\nI have created a version of the CMI 2022 Projection Model in R. This version solves the APCI model to produce the exact same mortality projections as the official Excel model. I am in the process of trying to release it to more actuaries (depending on the outcome of talks with the CMI committee).\nThere is currently an interactive dashboard connected to this model which can be explored here. A password is required for access, however it can be disclosed upon request."
  }
]