# function for cleaning LLMs
json_list_to_df <- function(output_list) {
  output_list |> purrr::map(function(x) {
    has_ticks <- stringr::str_detect(x, "```")
    if(has_ticks) {
      x <- x |> stringr::str_extract_all( "(?s)```(.*?)```") |> pluck(1) |> tail(1)
    }
    x |> 
      stringr::str_remove_all("\`") |>
      stringr::str_remove("json") |>
      jsonlite::fromJSON()
  }) |> dplyr::bind_rows()
}


#' Classify Data with LLM
#'
#' This function processes a dataset by chunking the data, sending each chunk to an LLM (Large Language Model) for classification, and returning the output.
#'
#' @param data A vector containing the data to be classified.
#' @param max_chunk_size An integer specifying the maximum size of each data chunk. Default is 15.
#' @param sleep_time_between_chunks Numeric. The amount of time (in seconds) to wait between processing each chunk. Default is 0.
#' @param chat_function The function used for the chat-based API call to the LLM. See ellmer package for choices
#' @param model A string indicating the model to be used for classification. 
#' @param message_updates Logical. Whether to print progress updates during processing. Default is TRUE.
#' @param intial_prompt The initial prompt to be sent to the LLM. Default is generated by calling `write_initial_prompt_v3(options)`.
#'
#' @return A list containing the LLM's responses for each data chunk
#' 
#' @examples
#' # Example usage
#' data <- c("data1", "data2", "data3")
#' result <- classify_data_with_llm(data)
#'
classify_data_with_llm <- 
function(
    data,
    max_chunk_size = 15,
    sleep_time_between_chunks = 0,
    chat_function = ellmer::chat_openai, 
    model = "gpt-4o-2024-08-06",
    message_updates = TRUE,
    intial_prompt = write_initial_prompt_v3(options)
  ) {

  # function for creating the chats
  glue_to_json <- function(vec) {
    glue::glue('[\n{glue::glue_collapse(glue::glue(\'  {{ "cause_of_death": "{vec}" }}\'), ",\n")}\n]')
  }
  
  # function for cleaning LLMs
  json_list_to_df <- function(output_list) {
    output_list |> purrr::map(function(x) {
      has_ticks <- stringr::str_detect(x, "```")
      if(has_ticks) {
        x <- x |> stringr::str_extract_all( "(?s)```(.*?)```") |> pluck(1) |> tail(1)
      }
      x |> 
        stringr::str_remove_all("\`") |>
        stringr::str_remove("json") |>
        jsonlite::fromJSON()
    }) |> dplyr::bind_rows()
  }
  
# get our prompts of cod_vector for processing
list_x <- cod_vector |> split_vector(max_chunk_size)
prompts_list <- list_x |> purrr::map(glue_to_json)
vectors <- seq_along(prompts_list)

# create output vector
llm_output <- vector("list", length = length(prompts_list))

# loop through and collect results!
for (i in vectors) {
  
  llm_chat <- do.call(chat_function, list(model = model, system_prompt = intial_prompt))
  
  llm_output[[i]] <- llm_chat$chat(prompts_list[[i]], echo = FALSE) 
  
  if(message_updates) {cli::cli_alert_info(glue::glue("completed {i} of {length(vectors)}"))}
  
  Sys.sleep(sleep_time_between_chunks)
}

output <- llm_output |> json_list_to_df()

return(output)
}
