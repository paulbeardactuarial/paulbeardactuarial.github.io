---
title: "Paul Beard FIA"
subtitle: "Life Actuary and Data Scientist"
format: 
  html:
    fig-pos: "H"
    include-in-header:
      - text: |
          <style>
          #tag-filter {
            margin-bottom: 20px;
            padding: 10px;
            background-color: #f0f0f0;
            border-radius: 5px;
            display: flex;
            flex-wrap: wrap;
            align-items: center;
            gap: 10px;
          }
          #tag-filter label {
            display: flex;
            align-items: center;
            gap: 5px;
          }
          </style>
fig-width: 8.5
fig-asp: 0.618
out-width: "85%" 
fig-align: center
tbl-cap-location: top
fig-cap-location: top
execute:
  echo: false
toc: true
---

```{r}
#| label: setup
#| include: false

library(tidyverse)
library(gt)
library(lubridate)
library(plotly)

data_loc <- "./paul.beard.for.hire/Data"
data_filename <- "Work History Summary.xlsx"

# Read in data
data_filepath <- paste(data_loc,"/",data_filename,sep="")
read_all_excel_sheets <- function(filename, tibble = FALSE) {
  sheets <- readxl::excel_sheets(filename)
  x <- lapply(sheets, function(X) readxl::read_excel(filename, sheet = X))
  if(!tibble) x <- lapply(x, as.data.frame)
  names(x) <- sheets
  x
}
cvdata <- read_all_excel_sheets(data_filepath)

```

-   I am a technically strong Fellow of the Institute of Actuaries working in the life insurance industry for `r {floor(interval(ISOdate(2012,9,3), now()) / years(1))}` years, with `r {floor(interval(ISOdate(2017,4,1), now()) / years(1))}` years post qualification.

-   I am an experienced coder. I have built complex models, complete packages and everything on this website. I have mastery over Excel and R, but am also adept across other platforms (see Software Experience). I understand good data and programming practices.

-   I have a fantastic track record of performance, both academically and in a work environment.

::: major-section-break
:::

# **Qualifications**

```{r}
#| label: qual-hist
#| warning: false

qual.hist <-
  tribble(
    ~Qualification,  ~`Awarded By`, ~Grade,  ~`Subject(s)`,      ~Date,
    "Fellowship",    "IFoA", NA,    "SA2, ST2, ST6",    "2017-12-01",
        "Masters", "Univ. of Cambridge", "2.i", "Natural Sciences (Chemistry)", "2011-07-01",
    "Bachelors","Univ. of Cambridge", "1", "Natural Sciences (Chemistry)", "2011-07-01"
  ) %>% 
  mutate(Date = as.Date(Date) %>% format('%b-%Y'))

qual.hist %>% 
  gt() %>% 
  tab_options(
        column_labels.background.color = "#008CBA",
        quarto.disable_processing = TRUE
        ) |>  
 tab_style_body(
    style = list(
      cell_fill(color = "grey95"),
      cell_text(color = "grey95")
      ),
      fn = \(x) is.na(x)
  )
```

Other achievements include:

-   One of top 5 scores in the UK for GCSE Maths (awarded by AQA in Aug 2005)

-   IFoA Certificate in Data Science (awarded by IFoA in Dec 2020)

::: major-section-break
:::

# **Software Experience**

The plot below demonstrates my career experience, both total accumulation and the relative recency. The figures are estimates as at 30/06/2025.

Please note the scale is logarithmic.

```{r}
#| label: fig-software-experience 
#| fig-cap: "Summary of Modelling Experience" 
#| cap-location: top 
#|  

experience <- cvdata$Software %>% rename_with(tolower)

exp_long <- experience %>%
  gather(key = year, value = hours, -"platform") %>%
  mutate(
    year = as.numeric(year),
    hours = as.numeric(hours)
  ) %>% 
    filter(!is.na(hours)) 

# get order of platforms in terms of total experience. These are platform_levels and used to order platforms within plot at the end
platform_levels <- exp_long %>%
  group_by(platform) %>%
  summarise(totes = sum(hours, na.rm = T)) %>%
  arrange(desc(totes)) %>%
  .$platform %>%
  rev()

years <- exp_long$year %>%  unique() %>% as.numeric() %>% sort()

# need to calculate marginal log hours for plot. Simple log transform of values will not work because log(sum(hours)) != sum(log(hours))
# NOTE we are arranging by highest year first when getting cumulative values... this is design choice, as most recent years are most relevant and so should look bigger
exp_plot_data <-
exp_long %>% 
  arrange(platform, desc(year)) %>% 
  group_by(platform) %>% 
  mutate(
    cum_hours = cumsum(hours),
    cum_log_hours = ifelse(cum_hours == 0, 0, log(cum_hours)),
    marginal_log_hours = diff(c(0, cum_log_hours), 1),
    platform = factor(platform, levels = platform_levels)
  ) 

#check no negative values
if(min(exp_plot_data$hours)<0) {stop("graph will distort because log hours include negative values")}

# set up x axis parameters
x.max <- exp_plot_data$cum_log_hours %>% max()
x.min <- 10 %>% log()
x.breaks <- c(10,100,1000,10000) %>% log()
x.labels <- exp(x.breaks)

# set up mid-year point for colour scale
mid.year <-  min(years) + 0.5*(max(years) - min(years)) 

# removing MG-Alfa as getting too cluttered
exp_plot_data <- exp_plot_data |> 
  filter(!platform %in% c("MG-Alfa"))

# plot of software experience (final output)
software_experience_plot <-
ggplot(exp_plot_data, aes(
  x = marginal_log_hours, 
  y = platform,
  fill = year,
  label = hours
)) +
  geom_col(width=0.62) +
  scale_fill_gradient2(
    name = "Year",
    breaks = scales::pretty_breaks(),
    low = "grey97",
    mid = "steelblue1",
    midpoint = mid.year,
    high = "steelblue4"
  ) +
  coord_cartesian(xlim = c(x.min, x.max)) +
  scale_x_continuous(
    name = "Estimated Cumulative Experience (Hours)",
    breaks = x.breaks,
    minor_breaks = NULL,
    labels = scales::comma(x.labels)
  ) +
  labs(caption = "**please note the scale is logarithmic") +
  ylab("Platform") +
  theme_classic() + 
  theme(panel.grid.major.x = element_line()) 


software_experience_plot %>% ggplotly(tooltip = c("y", "fill", "label")) 


```

::: major-section-break
:::

# **Work Experience**

:::: {#tag-filter-container}
<strong>Choose Your Own Adventure. What Work Experience Are You Interested In?</strong>

::: {#tag-filter}
:::
::::

```{=html}

<script>
document.addEventListener('DOMContentLoaded', function() {
  console.log('Script started'); // Debugging log
  // Collect all sections with tags
  const contentSections = document.querySelectorAll('[data-tags]');
  console.log(`Found ${contentSections.length} tagged sections`); // Debugging log
  
  // Define custom tag order
  const tagOrder = [
    'R Developer',
    'Data Scientist',
    'Annuity Pricing',
    'Commercial Real Estate Loan Rating',
    'Internal Model',
    'With Profits Management',
    'Mentor'
  ];

  // Collect unique tags
  const allTags = new Set();
  contentSections.forEach(section => {
    const tags = section.getAttribute('data-tags').split(',');
    tags.forEach(tag => {
      const trimmedTag = tag.trim();
      if (trimmedTag) allTags.add(trimmedTag);
    });
  });
  
  // Get filter container
  const filterContainer = document.getElementById('tag-filter');
  console.log(`Tags found: ${Array.from(allTags)}`); // Debugging log
  
  // Create a style element for custom checkbox styling
  const styleElement = document.createElement('style');
  styleElement.textContent = `
    #tag-filter {
      display: flex;
      flex-direction: column;
      align-items: flex-start;
    }
    #tag-filter input[type="checkbox"] {
      accent-color: #008CBA;
      margin-right: 10px;
    }
    #tag-filter label {
      display: flex;
      align-items: center;
      margin-bottom: 5px;
    }
  `;
  document.head.appendChild(styleElement);

  // Create checkboxes in the specified order
  tagOrder.forEach(tag => {
    // Only create checkbox if tag exists in document
    if (allTags.has(tag)) {
      // Create checkbox
      const checkbox = document.createElement('input');
      checkbox.type = 'checkbox';
      checkbox.id = `tag-${tag}`;
      checkbox.name = `tag-${tag}`;
      checkbox.value = tag;
      checkbox.checked = false;
      
      // Create label
      const label = document.createElement('label');
      label.htmlFor = `tag-${tag}`;
      
      // Add checkbox and text to label
      label.appendChild(checkbox);
      label.appendChild(document.createTextNode(tag));
      
      // Add to filter container
      filterContainer.appendChild(label);
    }
  });
  
  // Visibility update function
  function updateContentVisibility() {
    const activeFilters = Array.from(document.querySelectorAll('#tag-filter input:checked'))
      .map(cb => cb.value);
    console.log(`Active filters: ${activeFilters}`); // Debugging log
    
    // If no filters are selected, hide everything
    if (activeFilters.length === 0) {
      contentSections.forEach(section => {
        section.style.display = 'none';
      });
      return;
    }
    
    // Otherwise, show/hide based on selected tags
    contentSections.forEach(section => {
      const sectionTags = section.getAttribute('data-tags')
        .split(',')
        .map(tag => tag.trim())
        .filter(tag => tag !== '');
      
      // Show section if any of its tags are in active filters
      const shouldShow = sectionTags.some(tag => activeFilters.includes(tag));
      section.style.display = shouldShow ? '' : 'none';
      
      console.log(`Section tags: ${sectionTags}, Should show: ${shouldShow}`); // Debugging log
    });
  }
  
  // Add change event to all checkboxes
  document.querySelectorAll('#tag-filter input').forEach(checkbox => {
    checkbox.addEventListener('change', updateContentVisibility);
  });
  
  // Initial visibility update
  updateContentVisibility();
});
</script>
```

------------------------------------------------------------------------

::: {.content-section data-tags="Mentor, Data Scientist"}
### **Institute and Faculty of Actuaries** [(Volunteer, Various Dates)]{style="font-style: italic; font-size: 0.65em;"}
:::

::: {.content-section data-tags="Data Scientist"}
#### Data Science Working Party Member [(Jul 2025 â€” Present)]{style="font-style: italic; font-size: 0.65em;"}

-   Currently a member of the IFoA Data Science Working Party.

#### Life - Artificial Intelligence and Automation Working Party Member [(Jul 2025 â€” Present)]{style="font-style: italic; font-size: 0.65em;"}

-   Currently a member of the IFoA Life Artificial Intelligence and Automation Working Party
:::

::: {.content-section data-tags="Mentor"}
#### Student Employer Contact [(Oct 2018 â€” Oct 2021)]{style="font-style: italic; font-size: 0.65em;"}

-   Volunteered as a Student Employer Contact. This role involved engaging with the IFoA to facilitate exam result distribution, as well as helping shape IFoA policy around exams.

-   I advised many students during this time. I made sure to always be approachable to the students, and keep open paths of dialogue.
:::

### **Keenu** [(Start-up Colaborator, Jun 2025 â€” Present)]{style="font-style: italic; font-size: 0.65em;"}

#### Overview

-   I am working for an A.I. Tech Start-up, Keenu.io, which aims to create a simulated version of the IELTS (International English Language Testing System) speaking exam.

::: {.content-section data-tags="Data Scientist"}
#### Data Scientist and A.I. Engineer [(Jun 2025 â€” Present)]{style="font-style: italic; font-size: 0.65em;"}
:::

::: {.content-section data-tags="Data Scientist"}
-   One of the big challenges is getting an accurate score for the various facets of the speaking exam, based on the speech submitted by a student. To do this I have worked in Python, doing the following:
    -   Interacted with the APIs (Application Programming Interface) or SDKs (Software Development Kit) of various LLMs (Large Language Model) on the market
    -   Used extensive testing to rank LLM performance for each facet and determine company strategy.
    -   Performed fine-tuning of A.I. generated scores to increase their accuracy when compared to human scores.
-   The start-up only has four people working on it. The pace of change is fast, and strategy can change on a dime. As well as making sure models are as accurate as possible, I factored business needs into the decision making, such as cost, speed and materiality.
:::

### **SCOR** [(Employed as Contractor, Feb 2024 â€” Mar 2025)]{style="font-style: italic; font-size: 0.65em;"}

#### Overview

-   In my time at SCOR I was the lead R developer in a project to migrate the entire annuity pricing model suite from Excel / SAS / Word into to a single R workflow. This was a transformative project that streamlined longevity pricing at SCOR.

-   SCOR were impressed with my work and offered an extension after 6 months.

-   Management stated it was the first ever IT-based project they had known to finish on-time and on-budget.

::: {.content-section data-tags="Annuity Pricing, R Developer,Data Scientist"}
#### R Developer for Reinsurance Annuity Pricing Model Migration [(Feb 2024 â€” Mar 2025)]{style="font-style: italic; font-size: 0.65em;"}
:::

::: {.content-section data-tags="Annuity Pricing,R Developer"}
-   The initial project was completed for UK reinsurance pricing of BPA schemes. The project migrated the convoluted pricing process into a slick single R workflow. We revolutionised the pricing process at SCOR. I was the lead developer of an R package and Quarto book that together could perform all pricing calculations required, and produce html reports.
:::

::: {.content-section data-tags="Annuity Pricing,R Developer"}
-   The migration included building many complex pricing models in R:
    -   Pension Value Escalation model
    -   Proportion Married & Proportion Transferred Out derivation models
    -   Experience Analysis model
    -   Actual vs Expected Analysis and Adjustment model
    -   Net Present Value Cashflow model
    -   Risk Based Cashflow (i.e. Stressed Capital Requirement) models
:::

::: {.content-section data-tags="R Developer"}
-   As well as being able to develop complex actuarial models, the project entailed the following:
    -   Developing functions that are flexible, fast and documented via Roxygen
    -   Devloping a complete package of functions, data and tests that was rolled out to other actuaries
    -   Working with Git, Azure and Quarto
    -   Development of code to interact directly with an API of Club Vita to download mortality curves
    -   Consideration of usability, to develop a template that other actuaries, with less experience in R, could employ
    -   High quality visualisations including interactivity
    -   Extensive reconciliation and testing
:::

------------------------------------------------------------------------

### **Phoenix Group** [(Employed as Contractor, Apr 2023â€” Feb 2024)]{style="font-style: italic; font-size: 0.65em;"}

#### Overview

-   In my time at Phoenix I contributed to the assumptions structuring of the MG-Alfa capital model whilst working under the tight quarterly release cycle. I worked with incredibly complex spreadsheets that existed there, and often came up with innovative solutions to improve the neglected systems that were in place.

-   Phoenix were impressed with my work and offered an extension after 6 months.

::: {.content-section data-tags="Longevity, R Developer, Data Scientist"}
#### Assumptions Architect for BPAs [(April 2023 â€” Feb 2024)]{style="font-style: italic; font-size: 0.65em;"}
:::

::: {.content-section data-tags="R Developer, Data Scientist"}
-   Whilst at Phoenix I noted that there was considerable manual burden on checking spreadsheet changes. I therefore took the initiative to create a sophisticated comparison tool in R which could compare two complex assumptions spreadsheets.

-   By being able to replicate the tenets of how MG-Alfa reads table structures within R, the tool could produce an intelligent breakdown of assumption table changes which was superior to alternatives being used at the time (e.g. VBA cell-by-cell comparison, BeyondCompare). The tool was packaged and rolled out to other actuaries.
:::

------------------------------------------------------------------------

### **Royal London** [(Employed as Permanent, Sep 2012â€” Apr 2023)]{style="font-style: italic; font-size: 0.65em;"}

#### Overview

-   I worked at Royal London for over 10 years, gaining my fellowship as well as a vast array of experience across many actuarial sectors.

-   During that time I built up a strong reputation as someone who was technically astute and dependable. During appraisals I was consistently rated in the higher categories.

::: {.content-section data-tags="Commercial Real Estate Loan Rating, R Developer, Internal Model"}
#### Credit Quality Assessor of Commercial Real Estate Loans (CRELs) [(Aug 2021 â€” Apr 2023)]{style="font-style: italic; font-size: 0.65em;"}
:::

::: {.content-section data-tags="Commercial Real Estate Loan Rating"}
-   Royal London looked to expand the illiquid asset classes it invests in as part of its Matching Adjustment portfolio. This entailed the inclusion of CRELs. In this role I developed an internal credit quality assessment process (ICQA) for CRELs. As this asset class was new to the company, it required a lot of development from ground level.
:::

::: {.content-section data-tags="Commercial Real Estate Loan Rating, R Developer"}
-   As part of the role I created a complex stochastic model in R which captures the intricate cash flow structure of CRELs. The model simulates defaults across multiple correlated tenants over many time periods, and evaluates the outcomes of the loans to assess the credit quality.Â 
:::

::: {.content-section data-tags="Internal Model, Commercial Real Estate Loan Rating"}
-   After developing the ICQA process for CRELs, I then developed the internal model risk calibration for CREL risk.
:::

::: {.content-section data-tags="Mentor"}
#### Actuarial Student Overseer [(Jun 2018 â€” Nov 2021)]{style="font-style: italic; font-size: 0.65em;"}

-   Volunteered as coordinator and adviser to over 60 students sitting exams at Royal London. This role has required strong communication and leadership skills. I made sure to always be approachable to the students, and keep open paths of dialogue.

-   Within this role I collaborated with HR to construct company policy relating to students. I also dealt with many sensitive issues and delicate matters with complete discretion and professionalism.
:::

::: {.content-section data-tags="Internal Model"}
#### Internal Model Calibrator [(Mar 2018 â€” Aug 2021)]{style="font-style: italic; font-size: 0.65em;"}
:::

::: {.content-section data-tags="Internal Model"}
-   I worked on the development of the companyâ€™s Internal Model Application Process (IMAP) and produced complete insurance risk calibrations for Longevity Trend, Longevity Level, Mortality, Morbidity and Mortality Catastrophe risks.
:::

::: {.content-section data-tags="Internal Model"}
-   I also produced IMAP calibrations for Persistency, Commodity and Property risk. I also completed calibration of Market-to-market Dependencies looking at correlations between major risk groups.

-   Answered questions from both internal validators and the PRA about our IMAP methodology in a short period of time. Resolved validation findings in a timely manner.

-   I have led many workshops to co-ordinate the opinions of experts in their respective fields, and form consensus on the assumptions and methodology used in the internal model calibrations.
:::

::: {.content-section data-tags="Annuity Pricing, With Profits Management, Mentor"}
#### With Profits Management Actuary [(Sep 2012 â€” Mar 2018)]{style="font-style: italic; font-size: 0.65em;"}
:::

::: {.content-section data-tags="With Profits Management"}
-   Within this role I carried out regular analysis on bonus supportability for With Profits business. Royal London has a large portfolio of with profits business, often gained through acquisitions. Different business lines often has different methods/rules attached, giving me a broad and deep persepective on how with profits business can work.
:::

::: {.content-section data-tags="With Profits Management, Mentor"}
-   After many years working in with profits I became a subject matter expert, culminating in me leading a team of five junior actuarial staff to complete the year end final bonus reports.
:::

::: {.content-section data-tags="With Profits Management"}
-   I was involved in many with profits management tasks including (but not limited to) bonus setting, projections, run-off planning and estate distribution.

-   I have been involved in many bespoke projects, the most material being:

    -   Surrender value methodology development

    -   Prophet to MG-Alfa model reconciliation

    -   Policyholder remediation calculations
:::

::: {.content-section data-tags="With Profits Management, Annuity Pricing"}
-   I was a major contributor to Royal London's Guaranteed Annuity Option (GAO) compromise scheme. In this scheme, Royal London bought GAOs from policyholders in return for a material uplift to their asset share. My role was calculating a fair price for Royal London to offer the policyholders. This offer was submitted to the court and approved as a mass buyback, where policyholders were opted in by default but could choose to opt out.
:::

::: major-section-break
:::

# **Other Projects**

I write actuarial-based articles on my LinkedIn profile. I am currently in the process of migrating these to my website. Some highlights include:

-   [Artificial Actuary: Classifying Causes of Death](https://paulbeardactuarial.github.io/artificial_actuary_cod.html) An article that leverages the use of LLMs via an API connection to cleanse / categorise cause of death data.

-   [Coronation Street: The Most Dangerous Street in the UK?](https://www.linkedin.com/pulse/coronation-street-most-dangerous-uk-paul-beard/?trackingId=cJtp6%2BzBRiu9duF57XLXgw%3D%3D) An article that tries to make sense of the mortality rates experienced on the TV show 'Coronation Street'.

-   [Cryptocurrency: An Internal Model Risk Calibration](https://paulbeardactuarial.github.io/crypto_calibration.html) An article that demonstrates what an Internal Model Risk calibration might look like for a hypothetical insurer invested in non-trivial amounts of cryptocurrency assets.

I have created a version of the CMI 2022 Projection Model in R. This version solves the APCI model to produce the exact same mortality projections as the official Excel model. I am in the process of trying to release it to more actuaries (depending on the outcome of talks with the CMI committee).

There is currently an interactive dashboard connected to this model which can be explored [here](https://paulbeardactuarial.shinyapps.io/cmi_dashboard/). A password is required for access, however it can be disclosed upon request.
